{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNM02uksC233p+XMyLxWOFW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Beta628427/Tesis/blob/master/Beta_PCA/KPCA_RN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKZ9lrt1GnNp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ==============================================================================\n",
        "# Clase de capa de red\n",
        "\n",
        "class neural_layer():\n",
        "  def __init__(self, n_conn, n_neur, act_f):\n",
        "    self.act_f = act_f\n",
        "    self.b = np.random.rand(1, n_neur)      * 2 - 1\n",
        "    self.W = np.random.rand(n_conn, n_neur) * 2 - 1\n",
        "\n",
        "# ==============================================================================\n",
        "# Funciones de activaci√≥n\n",
        "\n",
        "# Sigmoido\n",
        "sigm = (lambda x: 1 / (1 + np.e ** (-x)),\n",
        "        lambda x: x * (1 - x))\n",
        "# Relu\n",
        "relu = lambda x: np.maximum(0, x)\n",
        "_x = np.linspace(-5, 5, 100)\n",
        "plt.plot(_x, relu(_x))\n",
        "\n",
        "# ==============================================================================\n",
        "# creamos red neurnal\n",
        "\n",
        "def create_nn(topology, act_f):\n",
        "  nn = []\n",
        "  for l, layer in enumerate(topology[:-1]):\n",
        "    nn.append(neural_layer(topology[l], topology[l+1], act_f))\n",
        "  return nn\n",
        "\n",
        "# ==============================================================================\n",
        "topology = [2, 8, 1]\n",
        "neural_net = create_nn(topology, sigm)  \n",
        "l2_cost = (lambda Yp, Yr: np.mean((Yp - Yr) ** 2),\n",
        "           lambda Yp, Yr: (Yp - Yr))\n",
        "def train(neural_net, X, Y, l2_cost, lr=0.5, train=True):\n",
        "  out = [(None, X)]\n",
        "  # Forward pass\n",
        "  for l, layer in enumerate(neural_net):\n",
        "    z = out[-1][1] @ neural_net[l].W + neural_net[l].b\n",
        "    a = neural_net[l].act_f[0](z)\n",
        "    out.append((z, a))\n",
        "  if train:\n",
        "    # Backward pass\n",
        "    deltas = []\n",
        "    for l in reversed(range(0, len(neural_net))):\n",
        "      z = out[l+1][0]\n",
        "      a = out[l+1][1]\n",
        "      if l == len(neural_net) - 1:\n",
        "        # calculamos delta de ultima capa\n",
        "        # formula de delta0\n",
        "        deltas.insert(0, l2_cost[1](a, Y) * neural_net[l].act_f[1](a))\n",
        "      else:\n",
        "        # calcular delta respecto a capa previa\n",
        "        deltas.insert(0, deltas[0] @ _W.T * neural_net[l].act_f[1](a))\n",
        "      _W = neural_net[l].W\n",
        "      # Gradient descent\n",
        "      neural_net[l].b = neural_net[l].b - np.mean(deltas[0], axis=0, \n",
        "                                                  keepdims=True) * lr   \n",
        "      neural_net[l].W = neural_net[l].W - out[l][1].T @ deltas[0] * lr\n",
        "  return out[-1][1]\n",
        "train(neural_net, X, Y, l2_cost, 0.5)\n",
        "print(\"\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}